# Purchase Predictor — Codebase Documentation

> **Purpose**: This document provides comprehensive documentation for the Purchase Predictor
> codebase. It is written so that another AI agent (or developer) can fully understand the
> architecture, reproduce the pipeline, and merge this codebase with another repository.

---

## Table of Contents

1. [Project Overview](#1-project-overview)
2. [Directory Structure](#2-directory-structure)
3. [Architecture & Data Flow](#3-architecture--data-flow)
4. [Feature Schema](#4-feature-schema)
5. [Module Documentation](#5-module-documentation)
   - 5.1 [generate_data.py](#51-generate_datapy)
   - 5.2 [generate_history.py](#52-generate_historypy)
   - 5.3 [find_danger_zones.py](#53-find_danger_zonespy)
   - 5.4 [train.py (XGBoost)](#54-trainpy-xgboost--primary)
   - 5.5 [train_sklearn_coreml.py](#55-train_sklearn_coremlpy)
   - 5.6 [convert.py (XGBoost → CoreML)](#56-convertpy-xgboost--coreml)
   - 5.7 [convert_sklearn_coreml.py](#57-convert_sklearn_coremlpy)
6. [Data Files](#6-data-files)
7. [Model Artifacts](#7-model-artifacts)
8. [Key Algorithms & Design Decisions](#8-key-algorithms--design-decisions)
9. [Dependencies](#9-dependencies)
10. [Build, Run & Deploy](#10-build-run--deploy)
11. [Configuration Reference](#11-configuration-reference)
12. [Technical Debt & Improvement Areas](#12-technical-debt--improvement-areas)
13. [Merging Guide for AI Agents](#13-merging-guide-for-ai-agents)

---

## 1. Project Overview

**Purchase Predictor** is a machine-learning pipeline that:

1. **Generates synthetic user-behavior data** (proximity, time-of-day, budget signals).
2. **Trains a binary classifier** (XGBoost) to predict whether a user will make a purchase.
3. **Converts the trained model to Apple CoreML** (`.mlmodel`) for on-device inference on iOS.
4. **Identifies "danger zones"** — geographic locations where a user historically regrets spending — and exports them as geofence coordinates for an iOS app.

The end goal is an **iOS "nudge" system**: when a user approaches a merchant and the model
predicts a high probability of purchase, the app can intervene with a spending nudge/alert,
especially in identified danger zones.

---

## 2. Directory Structure

```
pp_roots/                          ← Git repository root
├── .gitignore
└── purchase_predictor/            ← Project root
    ├── requirements.txt           ← Python dependencies
    ├── data/
    │   ├── synthetic_training_data.csv   ← Generated by generate_data.py
    │   ├── user_transaction_history.csv  ← Generated by generate_history.py
    │   └── danger_zones.json             ← Generated by find_danger_zones.py
    ├── models/
    │   ├── purchase_predictor.json       ← Trained XGBoost model (JSON format)
    │   ├── purchase_predictor_meta.json  ← Model metadata (features, threshold)
    │   └── PurchasePredictor.mlmodel     ← CoreML model for iOS
    └── src/
        ├── generate_data.py              ← Synthetic training data generator
        ├── generate_history.py           ← Synthetic transaction history generator
        ├── find_danger_zones.py          ← Danger zone identification from history
        ├── train.py                      ← XGBoost model training (primary)
        ├── train_sklearn_coreml.py       ← Logistic Regression alternative training
        ├── convert.py                    ← XGBoost → CoreML conversion (primary)
        └── convert_sklearn_coreml.py     ← Sklearn pipeline → CoreML conversion
```

---

## 3. Architecture & Data Flow

The pipeline consists of two independent tracks that share the same feature schema:

### Track A — Purchase Prediction Model

```
generate_data.py
    │
    ▼
data/synthetic_training_data.csv
    │
    ├──► train.py ──► models/purchase_predictor.json
    │                 models/purchase_predictor_meta.json
    │                         │
    │                         ▼
    │                 convert.py ──► models/PurchasePredictor.mlmodel ──► iOS App
    │
    └──► train_sklearn_coreml.py ──► models/purchase_predictor_sklearn.joblib
                                             │
                                             ▼
                                     convert_sklearn_coreml.py ──► models/PurchasePredictor.mlmodel
```

### Track B — Danger Zone Detection

```
generate_history.py
    │
    ▼
data/user_transaction_history.csv
    │
    ▼
find_danger_zones.py
    │
    ▼
data/danger_zones.json ──► iOS App (geofence coordinates)
```

### Execution Order

Scripts must be run in dependency order:

| Step | Script                              | Depends On                          |
|------|-------------------------------------|-------------------------------------|
| 1a   | `python src/generate_data.py`       | —                                   |
| 1b   | `python src/generate_history.py`    | —                                   |
| 2a   | `python src/train.py`               | Step 1a (training data CSV)         |
| 2b   | `python src/find_danger_zones.py`   | Step 1b (transaction history CSV)   |
| 3    | `python src/convert.py`             | Step 2a (XGBoost JSON model + meta) |

Steps 1a/1b are independent and can run in parallel. Steps 2a/2b are independent of each other.

---

## 4. Feature Schema

All models use the same 6 input features. **Feature order matters** — it must be
consistent between training, conversion, and iOS inference.

| # | Feature Name            | Type    | Range       | Description                                    |
|---|-------------------------|---------|-------------|------------------------------------------------|
| 1 | `distance_to_merchant`  | Integer | 0–500       | Distance to the merchant in meters              |
| 2 | `hour_of_day`           | Integer | 0–23        | Hour of the day (24h format)                    |
| 3 | `is_weekend`            | Binary  | 0 or 1      | Whether the current day is a weekend            |
| 4 | `budget_utilization`    | Float   | 0.0–1.0     | Fraction of monthly budget already spent        |
| 5 | `merchant_regret_rate`  | Float   | 0.0–1.0     | Historical regret rate for this merchant        |
| 6 | `dwell_time`            | Integer | 0–600       | Time spent near the merchant in seconds         |

**Target variable**: `purchase_occurred` (binary: 0 = no purchase, 1 = purchase).

---

## 5. Module Documentation

### 5.1 `generate_data.py`

**Purpose**: Creates a synthetic labeled dataset for training the purchase prediction model.

**Inputs**: None (self-contained generator).

**Outputs**: `data/synthetic_training_data.csv` — 10,000 rows, 7 columns (6 features + 1 target).

**Algorithm — Labeling Logic**:
A deterministic-with-noise scoring function assigns the `purchase_occurred` label:

```
score = 0.0
if merchant_regret_rate > 0.7  → score += 0.4
if hour_of_day > 20            → score += 0.2
if budget_utilization > 0.8    → score += 0.3
if distance_to_merchant < 50   → score += 0.2

probability = clamp(score + uniform(-0.1, 0.1), 0, 1)
label = 1 if probability > 0.6 else 0
```

The maximum score without noise is 1.1 (clamped to 1.0). The noise range of ±0.1
creates realistic label uncertainty near decision boundaries.

**Design note**: The `dwell_time` and `is_weekend` features are generated but **not used**
in the labeling logic. They exist as realistic feature noise; the model must learn to
(mostly) ignore them.

**Dependencies**: `pandas`, `numpy`, `random`, `pathlib`.

**Path resolution**: Uses `Path(__file__).resolve().parents[1]` — safe to run from any directory.

---

### 5.2 `generate_history.py`

**Purpose**: Creates a synthetic 3-month transaction history for a single user, used for
danger zone detection.

**Inputs**: None (self-contained generator).

**Outputs**: `data/user_transaction_history.csv` — ~50 rows.

**Schema**:

| Column     | Type   | Description                              |
|------------|--------|------------------------------------------|
| `merchant` | String | Merchant name                            |
| `amount`   | Float  | Transaction amount in dollars            |
| `date`     | String | Date in `YYYY-MM-DD` format             |
| `hour`     | Int    | Hour of transaction                      |
| `lat`      | Float  | Latitude of merchant                     |
| `lng`      | Float  | Longitude of merchant                    |
| `regret`   | Bool   | Whether the user regretted this purchase |

**Hardcoded locations** (simulating Pittsburgh, PA):

| Merchant        | Category  | Risk    | Regret Logic                    |
|-----------------|-----------|---------|----------------------------------|
| The Dive Bar    | Nightlife | High    | Always regretted, late night     |
| Whole Foods     | Grocery   | Low     | Never regretted, afternoon       |
| Tech Store      | Shopping  | Medium  | 50/50 regret, random amounts     |

**Dependencies**: `pandas`, `numpy`, `random`, `datetime`.

**Path resolution**: Uses relative path `"data/..."` — **must be run from project root**
(`purchase_predictor/`).

---

### 5.3 `find_danger_zones.py`

**Purpose**: Analyzes transaction history to identify geographic "danger zones" where the
user frequently regrets spending.

**Inputs**: `data/user_transaction_history.csv` (from `generate_history.py`).

**Outputs**: `data/danger_zones.json` — array of objects with `merchant`, `lat`, `lng`,
`regret_count`.

**Algorithm**:
1. Filter transactions where `regret == True`.
2. Group by `(merchant, lat, lng)` and count occurrences.
3. Apply threshold: keep locations with `regret_count >= 1`.
4. Export as JSON (array of records).

**Output JSON schema**:
```json
[
  {
    "merchant": "The Dive Bar",
    "lat": 40.444,
    "lng": -79.943,
    "regret_count": 17
  }
]
```

**iOS integration**: These coordinates are intended to be used as CLCircularRegion
geofences in an iOS app to trigger proximity alerts.

**Dependencies**: `pandas`.

**Path resolution**: Relative paths — **must be run from project root**.

---

### 5.4 `train.py` (XGBoost — Primary)

**Purpose**: Trains the primary XGBoost binary classifier for purchase prediction.

**Inputs**: `data/synthetic_training_data.csv`.

**Outputs**:
- `models/purchase_predictor.json` — XGBoost model in JSON format.
- `models/purchase_predictor_meta.json` — Metadata (feature names, threshold, notes).

**Model configuration**:

| Hyperparameter     | Value    | Rationale                              |
|--------------------|----------|----------------------------------------|
| `max_depth`        | 6        | Moderate tree depth                    |
| `learning_rate`    | 0.05     | Slow learning for better generalization|
| `n_estimators`     | 200      | Enough capacity for 6-feature problem  |
| `subsample`        | 0.9      | Row sampling to reduce overfitting     |
| `colsample_bytree` | 0.9      | Column sampling to reduce overfitting  |
| `reg_lambda`       | 1.0      | L2 regularization                      |
| `eval_metric`      | logloss  | Standard for binary classification     |
| `random_state`     | 42       | Reproducibility                        |

**Evaluation pipeline**:
1. 80/20 stratified train/test split (preserves class ratio).
2. Predicts **probabilities** (not hard labels).
3. Applies configurable threshold (default `0.70`) to convert probabilities → binary.
4. Reports: Accuracy, Precision, Recall, F1, AUC, Confusion Matrix, Classification Report.
5. Runs a **threshold sweep** at `[0.50, 0.60, 0.70, 0.80, 0.90]` to help choose the
   best nudge threshold.
6. Includes a **demo prediction** with a single hardcoded example.

**Metadata JSON schema** (`purchase_predictor_meta.json`):
```json
{
  "model_type": "xgboost",
  "feature_names": ["distance_to_merchant", "hour_of_day", ...],
  "threshold": 0.7,
  "notes": "Probability threshold used for nudges. Keep feature order consistent at inference."
}
```

**Dependencies**: `pandas`, `xgboost`, `sklearn`, `json`, `os`.

**Path resolution**: Relative paths — **must be run from project root** (`purchase_predictor/`).

---

### 5.5 `train_sklearn_coreml.py`

**Purpose**: Alternative training pipeline using scikit-learn's `LogisticRegression`
wrapped in a `Pipeline` with `StandardScaler`. This produces a model that converts to
CoreML more straightforwardly via `coremltools.converters.sklearn`.

**Inputs**: `data/synthetic_training_data.csv`.

**Outputs**: `models/purchase_predictor_sklearn.joblib` — serialized sklearn Pipeline.

**Model configuration**:
- `StandardScaler` → `LogisticRegression(max_iter=2000, class_weight="balanced")`
- Same 0.70 threshold, same evaluation metrics as `train.py`.
- `class_weight="balanced"` adjusts for class imbalance automatically.

**Dependencies**: `pandas`, `sklearn`, `joblib`, `pathlib`.

**Path resolution**: Uses `Path(__file__).resolve().parent.parent` — safe from any directory.

---

### 5.6 `convert.py` (XGBoost → CoreML)

**Purpose**: Converts the trained XGBoost model to Apple CoreML `.mlmodel` format for
on-device inference on iOS/macOS.

**Inputs**:
- `models/purchase_predictor.json` — Trained XGBoost Booster.
- `models/purchase_predictor_meta.json` — Feature names (for correct input ordering).

**Outputs**: `models/PurchasePredictor.mlmodel` — CoreML model file.

**Conversion details**:
- Loads the model as `xgb.Booster()` (not the sklearn wrapper) for maximum compatibility
  with `coremltools`.
- Uses `ct.converters.xgboost.convert()` with explicit parameters:
  - `feature_names` — read from metadata JSON (ensures correct order).
  - `mode="classifier"` — binary classification output.
  - `class_labels=[0, 1]` — 0 = No Purchase, 1 = Purchase.
  - `force_32bit_float=True` — smaller model size for mobile.
- Adds Xcode-visible metadata: `author`, `short_description`, per-feature `input_description`.

**CoreML model interface** (as seen in Xcode):

| Input                  | Type   | Description                       |
|------------------------|--------|-----------------------------------|
| distance_to_merchant   | Double | Distance to merchant in meters    |
| hour_of_day            | Double | Hour of the day (0-23)            |
| is_weekend             | Double | Whether it is a weekend (0 or 1)  |
| budget_utilization     | Double | Budget utilization ratio (0-1)    |
| merchant_regret_rate   | Double | Merchant regret rate (0-1)        |
| dwell_time             | Double | Dwell time in seconds             |

| Output                 | Type       | Description                     |
|------------------------|------------|---------------------------------|
| purchase_occurred      | Int64      | Predicted class (0 or 1)        |
| classProbability       | Dictionary | {0: prob, 1: prob}              |

**Dependencies**: `coremltools`, `xgboost`, `json`, `pathlib`.

**Path resolution**: Uses `Path(__file__).resolve().parents[1]` — safe from any directory.

---

### 5.7 `convert_sklearn_coreml.py`

**Purpose**: Converts the sklearn `Pipeline` (Scaler + LogisticRegression) to CoreML.

**Inputs**: `models/purchase_predictor_sklearn.joblib`.

**Outputs**: `models/PurchasePredictor.mlmodel` (overwrites the same path as `convert.py`).

**Conversion details**:
- Uses `ct.converters.sklearn.convert()`.
- Specifies input features with explicit `ct.models.datatypes.Double()` types.

**Note**: This will **overwrite** the XGBoost CoreML model if both converters target the
same output path. Choose one pipeline (XGBoost or sklearn) before deploying.

**Dependencies**: `coremltools`, `joblib`, `pathlib`.

---

## 6. Data Files

| File                             | Generated By          | Format | Rows    | Description                        |
|----------------------------------|-----------------------|--------|---------|------------------------------------|
| `synthetic_training_data.csv`    | `generate_data.py`    | CSV    | 10,000  | Training data (6 features + label) |
| `user_transaction_history.csv`   | `generate_history.py` | CSV    | ~50     | Simulated spending history         |
| `danger_zones.json`              | `find_danger_zones.py`| JSON   | ~3      | Geofence coordinates for iOS       |

All data is synthetic. No real user data is present.

---

## 7. Model Artifacts

| File                              | Format   | Producer               | Consumer              |
|-----------------------------------|----------|------------------------|-----------------------|
| `purchase_predictor.json`         | XGBoost JSON | `train.py`         | `convert.py`          |
| `purchase_predictor_meta.json`    | JSON     | `train.py`             | `convert.py`, iOS app |
| `purchase_predictor_sklearn.joblib`| Joblib  | `train_sklearn_coreml.py` | `convert_sklearn_coreml.py` |
| `PurchasePredictor.mlmodel`       | CoreML   | `convert.py` or `convert_sklearn_coreml.py` | iOS/macOS app |

---

## 8. Key Algorithms & Design Decisions

### 8.1 Why XGBoost as Primary?

- **Performance**: Gradient-boosted trees handle the tabular 6-feature problem well with
  minimal tuning.
- **CoreML support**: `coremltools` has a dedicated `ct.converters.xgboost.convert()`
  converter for tree ensembles.
- **Interpretability**: Feature importance is directly available from XGBoost.

### 8.2 Why a Logistic Regression Alternative?

- **Simpler CoreML model**: Fewer parameters, faster on-device inference.
- **Fallback**: If XGBoost conversion hits compatibility issues, the sklearn pipeline is a
  safe alternative.
- **Balanced classes**: Uses `class_weight="balanced"` to handle label imbalance without
  manual threshold tuning.

### 8.3 Probability Threshold (0.70)

The model outputs probabilities, not hard labels. A threshold of 0.70 is applied to
decide whether to send a "nudge":
- **Higher threshold** (e.g., 0.90) → fewer nudges, higher precision (fewer false alarms).
- **Lower threshold** (e.g., 0.50) → more nudges, higher recall (catch more purchases).
- The threshold is stored in `purchase_predictor_meta.json` and should be applied on the
  iOS side when interpreting `classProbability` from the CoreML model.

### 8.4 Danger Zone Detection

A simple group-by-count approach over regret-flagged transactions. No ML involved —
purely rule-based. The threshold (`regret_count >= 1`) is intentionally low for
demonstration purposes.

---

## 9. Dependencies

### Python Packages (`requirements.txt`)

| Package        | Version Constraint | Purpose                               |
|----------------|-------------------|---------------------------------------|
| `pandas`       | latest            | Data loading and manipulation         |
| `numpy`        | latest            | Numerical operations, data generation |
| `scikit-learn` | latest            | Train/test split, metrics, sklearn models |
| `xgboost`      | >= 2.0.0          | Primary model training (pre-built wheels required) |
| `coremltools`  | latest            | CoreML model conversion               |

### Additional implicit dependencies

| Package  | Pulled In By   | Purpose                     |
|----------|----------------|-----------------------------|
| `joblib` | `scikit-learn` | Model serialization (sklearn)|
| `scipy`  | `scikit-learn` | Numerical routines           |

### System Requirements

- **Python**: 3.10+ (tested with 3.11 and 3.12).
- **macOS**: Required for CoreML conversion (coremltools only runs on macOS/Linux, but
  `.mlmodel` is only usable on Apple platforms).
- **No GPU required**: XGBoost runs on CPU with these settings.

---

## 10. Build, Run & Deploy

### Initial Setup

```bash
cd purchase_predictor
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### Full Pipeline Execution

```bash
# From purchase_predictor/ directory:

# Step 1: Generate all synthetic data
python src/generate_data.py
python src/generate_history.py

# Step 2: Train model + find danger zones
python src/train.py
python src/find_danger_zones.py

# Step 3: Convert to CoreML
python src/convert.py
```

### Output Verification

After running the full pipeline, you should have:

```
models/
├── purchase_predictor.json         ← XGBoost model (~525 KB)
├── purchase_predictor_meta.json    ← Metadata with features + threshold
└── PurchasePredictor.mlmodel       ← CoreML model (drag into Xcode)

data/
├── synthetic_training_data.csv     ← 10K rows training data
├── user_transaction_history.csv    ← 50 rows transaction history
└── danger_zones.json               ← Geofence coordinates
```

### iOS Integration

1. Drag `PurchasePredictor.mlmodel` into your Xcode project.
2. Xcode auto-generates a Swift class `PurchasePredictor` with typed inputs/outputs.
3. At inference time, create a `PurchasePredictorInput` with the 6 features.
4. Read `classProbability[1]` and compare against `threshold` from the metadata JSON.
5. Load `danger_zones.json` to create `CLCircularRegion` geofences.

---

## 11. Configuration Reference

All configurable values with their locations:

| Parameter         | Value   | File                    | Line(s)  | Description                    |
|-------------------|---------|-------------------------|----------|--------------------------------|
| `N_SAMPLES`       | 10000   | `generate_data.py`      | 7        | Training data size             |
| `DEFAULT_THRESHOLD`| 0.70   | `train.py`              | 26       | Nudge probability threshold    |
| `test_size`       | 0.2     | `train.py`              | 49       | Train/test split ratio         |
| `max_depth`       | 6       | `train.py`              | 58       | XGBoost tree depth             |
| `n_estimators`    | 200     | `train.py`              | 60       | Number of boosting rounds      |
| `learning_rate`   | 0.05    | `train.py`              | 59       | XGBoost learning rate          |
| `regret threshold`| >= 1    | `find_danger_zones.py`  | 16       | Min regrets to flag a zone     |
| Transaction count | 50      | `generate_history.py`   | 18       | Number of synthetic transactions|

---

## 12. Technical Debt & Improvement Areas

### Path Inconsistency (High Priority)

- `generate_data.py` and `convert.py` use `Path(__file__).resolve().parents[1]` (robust).
- `train.py`, `generate_history.py`, and `find_danger_zones.py` use **relative paths**
  like `"data/..."` which only work when run from the `purchase_predictor/` directory.
- **Fix**: Standardize all scripts to use `Path(__file__).resolve().parents[1]` for ROOT.

### Two Competing Pipelines

- Both `convert.py` and `convert_sklearn_coreml.py` write to `PurchasePredictor.mlmodel`.
- The last one to run wins. No versioning or naming distinction.
- **Fix**: Use distinct output names (e.g., `PurchasePredictor_XGB.mlmodel` vs
  `PurchasePredictor_LR.mlmodel`) or consolidate into one pipeline.

### No Automated Tests

- No unit tests, integration tests, or model validation scripts.
- **Fix**: Add `pytest` tests for data generation (schema validation), model training
  (metric thresholds), and CoreML conversion (round-trip prediction check).

### Synthetic Data Only

- All data is generated synthetically. The labeling logic in `generate_data.py` defines
  the "ground truth" — the model is essentially learning a known function.
- **Fix**: When real data becomes available, replace `generate_data.py` with a data
  loading module and re-train.

### No Model Versioning

- Models are overwritten in place. No version tracking or experiment logging.
- **Fix**: Add MLflow, DVC, or at minimum timestamp-based output directories.

### Missing `joblib` in requirements.txt

- `train_sklearn_coreml.py` imports `joblib` directly. While it ships with `scikit-learn`,
  it should be listed explicitly if used as a direct import.

### Threshold Not Embedded in CoreML

- The 0.70 threshold is stored in the metadata JSON but **not** in the CoreML model itself.
- The iOS app must read the threshold separately or hardcode it.
- **Fix**: Consider building a CoreML pipeline that includes the thresholding step, or
  document the threshold contract clearly for the iOS team.

### No Logging

- All scripts use `print()` for output. No structured logging.
- **Fix**: Use Python's `logging` module for production scripts.

---

## 13. Merging Guide for AI Agents

If you are an AI agent merging this codebase with another repository, follow these rules:

### Critical Contracts

1. **Feature order must be preserved.** The 6 features listed in
   `purchase_predictor_meta.json` must appear in the exact same order during training,
   conversion, and iOS inference. Reordering will silently produce wrong predictions.

2. **The CoreML model expects all 6 inputs as `Double`.** No integer or categorical
   encoding is needed — pass raw numeric values.

3. **The `.mlmodel` output is a tree ensemble classifier.** It produces two outputs:
   `purchase_occurred` (Int64, the predicted class) and `classProbability` (Dictionary,
   per-class probabilities). Use `classProbability[1]` for the nudge probability.

4. **The metadata JSON is the source of truth** for feature names, threshold, and model
   type. Always read it programmatically rather than hardcoding values.

### File Dependencies

```
generate_data.py  →  synthetic_training_data.csv  →  train.py  →  purchase_predictor.json
                                                                    purchase_predictor_meta.json
                                                                         ↓
                                                                    convert.py  →  PurchasePredictor.mlmodel
```

If you move or rename any file in this chain, update all downstream references.

### Safe Modifications

- Hyperparameters in `train.py` can be changed freely — re-run the full pipeline afterward.
- New features can be added to `generate_data.py` and the feature list will auto-propagate
  through `train.py` → metadata JSON → `convert.py`. However, you must also update the
  `input_description` block in `convert.py` and the iOS model input struct.
- The threshold can be changed in `train.py` (line 26). It propagates to metadata JSON
  automatically on the next training run.

### Dangerous Modifications

- **Do not change feature order** without retraining and reconverting.
- **Do not change the target column name** (`purchase_occurred`) without updating
  `train.py`, `convert.py`, and iOS inference code.
- **Do not mix XGBoost and sklearn CoreML models** — they have different output schemas.
  Pick one pipeline and be consistent.
